# CLIP Fusion configuration for MMHS150K dataset

# Inherit from default
_base_: default.yaml

# Model
model:
  backend: clip
  head: fusion
  encoder_name: openai/clip-vit-base-patch32
  fusion_dim: 512

# Training
training:
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 2
  num_train_epochs: 8
  lr_encoder: 1.0e-5
  lr_head: 5.0e-4

# Output
saving:
  output_dir: runs/clip_fusion_mmhshateful
